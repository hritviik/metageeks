from pyspark.sql import SparkSession
from pyspark.sql.functions import col, length
import json
import math
import boto3
import io
import re
import pymysql
import os
from datetime import datetime
from awsglue.context import GlueContext
from pyspark.context import SparkContext


def read_parquet_file(spark, path):
    return spark.read.parquet(path)


def get_null_ratio(data, field, total_rows):
    null_count = data.filter(data[field].isNull()).count()
    return null_count / total_rows


def get_uniqueness_percentage(data, field, total_rows):
    distinct_count = data.select(field).distinct().count()
    return (distinct_count / total_rows) * 100


def get_min_value(data, field):
    return data.agg({field: "min"}).collect()[0][0]


def get_max_value(data, field):
    return data.agg({field: "max"}).collect()[0][0]


def get_min_length(data, field):
    return data.select(length(col(field))).agg({f"length({field})": "min"}).collect()[0][0]


def get_max_length(data, field):
    return data.select(length(col(field))).agg({f"length({field})": "max"}).collect()[0][0]


def get_avg_length(data, field):
    return data.select(length(col(field))).agg({f"length({field})": "avg"}).collect()[0][0]


def calculate_storage(data_type, avg_length, storage_dict):
    if data_type == "IntegerType":
        min_storage = 0
        max_storage = storage_dict[data_type]
    elif data_type == "StringType":
        min_storage = 0
        max_storage = avg_length
    elif data_type == "TimestampType":
        min_storage = 0
        max_storage = storage_dict[data_type]
    else:
        min_storage = 0
        max_storage = 0
    return min_storage, max_storage


def get_distinct_count(data, field):
    distinct_count = 0
    pattern_char = r'[a-zA-Z]'
    pattern_num = r'\d'
    pattern_special = r'[!@#$%^&*"(),. ?\:{}|<>]'
    onlychar = 0
    onlynum = 0
    charnum = 0
    charsp = 0
    numsp = 0
    all3 = 0

    for row in data.select(field).collect():
        string = str(row[field])

        if string.isalpha() and onlychar == 0:
            distinct_count += 1
            onlychar = 1
        elif string.isdigit() and onlynum == 0:
            distinct_count += 1
            onlynum = 1
        elif bool(re.search(pattern_char, string)) and bool(re.search(pattern_num, string)) and not bool(
                re.search(pattern_special, string)) and charnum == 0:
            distinct_count += 1
            charnum = 1
        elif bool(re.search(pattern_char, string)) and bool(re.search(pattern_special, string)) and not bool(
                re.search(pattern_num, string)) and charsp == 0:
            distinct_count += 1
            charsp = 1
        elif bool(re.search(pattern_special, string)) and bool(re.search(pattern_num, string)) and not bool(
                re.search(pattern_char, string)) and numsp == 0:
            distinct_count += 1
            numsp = 1
        elif bool(re.search(pattern_special, string)) and bool(re.search(pattern_char, string)) and bool(
                re.search(pattern_num, string)) and all3 == 0:
            distinct_count += 1
            all3 = 1

    return distinct_count


dataProfilerSparkSession = SparkSession.builder.appName("NullValueCounter").getOrCreate()

# Create a GlueContext
glueContext = GlueContext(dataProfilerSparkSession)

json_dynamic_frame = glueContext.create_dynamic_frame_from_options("s3", {
    'paths': ["s3://metageeks-dataprofile-params/dataprofile-input-params.json"], 'recurse': True,
    'groupFiles': 'inPartition', 'groupSize': '1048576'}, format="json")
json_data_frame = json_dynamic_frame.toDF()
inputDataObjParameter = json_data_frame.select(col("dataObjectName")).first()[0]
dataFrameForProfile = read_parquet_file(dataProfilerSparkSession, inputDataObjParameter)
# if ".csv" in path:
filename = os.path.basename(inputDataObjParameter[0:len(inputDataObjParameter) - 1])
# else:
#      filename_parts =path.split("//")[1].split("/")[1]+path.split("//")[1].split("/")[2]+path.split("//")[1].split("/")[3]
#      filename =filename_parts

print(filename)
schema = dataFrameForProfile.schema
# Connect to the RDS database
endpoint = "database-1.codti4vj2iqk.us-east-2.rds.amazonaws.com"
username = "admin12"
password = "Capgemini#12"
database_name = "mg-data-profile-db"
connection = pymysql.connect(host=endpoint, user=username, passwd=password, db=database_name)
cursor = connection.cursor()

# Execute a SELECT query to fetch data from a specific table
table_name1 = "storage_type"
query = f"SELECT * FROM {table_name1}"
cursor.execute(query)

# Fetch all the rows returned by the query
rows = cursor.fetchall()
storage_dict = {}
for row in rows:
    # Access specific column values using index or column names
    storage_dict[row[0]] = row[1]

schema_dict = {}
column_uniqueness = {}
cntr = 1
total_rows = dataFrameForProfile.count()
min_storage_sum = 0
max_storage_sum = 0

for field in schema.fields:
    result = {}
    result["columnPosition"] = cntr
    result["dataType"] = str(field.dataType)
    result["nullRatio"] = get_null_ratio(dataFrameForProfile, field.name, total_rows)
    result["uniquenessPercentage"] = get_uniqueness_percentage(dataFrameForProfile, field.name, total_rows)

    if "date" in result["dataType"].lower() or "integerType" in result["dataType"].lower():
        result["minValue"] = get_min_value(dataFrameForProfile, field.name)
        result["minValuePercentage"] = dataFrameForProfile.filter(
            dataFrameForProfile[field.name] == result["minValue"]).count() / total_rows * 100
        result["maxValue"] = get_max_value(dataFrameForProfile, field.name)
        result["maxValuePercentage"] = dataFrameForProfile.filter(
            dataFrameForProfile[field.name] == result["maxValue"]).count() / total_rows * 100
    else:
        result["minValue"] = "NA"
        result["maxValue"] = "NA"

    result["minLength"] = get_min_length(dataFrameForProfile, field.name)
    result["maxLength"] = get_max_length(dataFrameForProfile, field.name)
    result["avgLength"] = get_avg_length(dataFrameForProfile, field.name)

    min_storage, max_storage = calculate_storage(result["dataType"], result["avgLength"], storage_dict)
    result["minStorage(Bytes)"] = min_storage
    result["maxStorage(Bytes)"] = max_storage

    result["distinctCount"] = get_distinct_count(dataFrameForProfile, field.name)
    schema_dict[field.name] = result
    cntr += 1
    min_storage_sum += min_storage
    max_storage_sum += max_storage

schema_dict["minStorage(Bytes)"] = min_storage_sum
schema_dict["maxStorage(Bytes)"] = max_storage_sum

# Print the schema dictionary
print(schema_dict)

json_output = json.dumps(schema_dict, indent=4)

# Store the JSON output in an S3 bucket
bucket_name = "metageeks-data-profile-results"
output_path = "inferschema_result"
file_extension = "json"
s3 = boto3.client("s3")
s3.put_object(Bucket=bucket_name, Key=output_path + "." + file_extension, Body=json_output)

# Fetch the current date
now = datetime.now()
current_date = now.strftime("%Y-%m-%d %H:%M:%S")

# Insert the JSON output into the data_profile_result table and data_profiler
cursor = connection.cursor()
json1 = json.loads(json_output)
del json1['minStorage(Bytes)']

del json1['maxStorage(Bytes)']
print("BY H", json1)
for i in json1:
    colpos = json1[i]['columnPosition']
    datatype = json1[i]['dataType']
    del json1[i]['columnPosition']
    del json1[i]['dataType']

    cursor.execute(
        "INSERT INTO data_profile_result (run_date, run_data_object_name, Column_profile_result, column_name, column_position, column_datatype) VALUES (%s, %s, %s, %s, %s, %s)",
        (current_date, filename, json.dumps(json1[i]), i, colpos, datatype)
    )

    connection.commit()

# # Check if the values can be converted to float
# min_storage = float(min_storage_sum) if isinstance(min_storage_sum, (int, float)) else None
# max_storage = float(max_storage_sum) if isinstance(max_storage_sum, (int, float)) else None

summary_json = {
    "avgRowLen(Bytes)": math.ceil(max_storage_sum)
}
summary_json_str = json.dumps(summary_json)

# Insert the summary JSON output into the profile_summary table
cursor.execute(
    "INSERT INTO profile_summary (run_date, data_object_name, profile_summary) VALUES (%s, %s, %s)",
    (current_date, filename, summary_json_str)
)

connection.commit()

# Close the database connection
connection.close()

